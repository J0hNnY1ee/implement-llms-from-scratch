{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, os, urllib\n",
    "import urllib.request\n",
    "# 设置代理\n",
    "os.environ[\"http_proxy\"] = \"http://127.0.0.1:7890\"\n",
    "os.environ[\"https_proxy\"] = \"http://127.0.0.1:7890\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of entries: 1100\n",
      "Example entry:\n",
      " {'instruction': 'Identify the correct spelling of the following word.', 'input': 'Ocassion', 'output': \"The correct spelling is 'Occasion.'\"}\n",
      "Another example entry:\n",
      " {'instruction': \"What is an antonym of 'complicated'?\", 'input': '', 'output': \"An antonym of 'complicated' is 'simple'.\"}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import json, os, urllib\n",
    "import urllib.request\n",
    "# 下载数据集并加载\n",
    "def download_and_load_file(file_path, url):\n",
    "\n",
    "    if not os.path.exists(file_path):\n",
    "        with urllib.request.urlopen(url) as response:\n",
    "            text_data = response.read().decode(\"utf-8\")\n",
    "        with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "            file.write(text_data)\n",
    "    else:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            text_data = file.read()\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        data = json.load(file)\n",
    "        \n",
    "    return data\n",
    "\n",
    "file_path = \"instruction-data.json\"\n",
    "url = (\n",
    "    \"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch\"\n",
    "    \"/main/ch07/01_main-chapter-code/instruction-data.json\"\n",
    ")\n",
    "\n",
    "data = download_and_load_file(file_path, url)\n",
    "print(\"Number of entries:\", len(data))\n",
    "print(\"Example entry:\\n\", data[50])\n",
    "print(\"Another example entry:\\n\", data[999])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Identify the correct spelling of the following word.\n",
      "\n",
      "### Input:\n",
      "Ocassion\n",
      "\n",
      "### Response:\n",
      "The correct spelling is 'Occasion.'\n",
      "\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "What is an antonym of 'complicated'?\n",
      "\n",
      "### Response:\n",
      "An antonym of 'complicated' is 'simple'.\n"
     ]
    }
   ],
   "source": [
    "# 需要格式化输入\n",
    "def format_input(entry):\n",
    "    instruction_text = (\n",
    "        f\"Below is an instruction that describes a task. \"\n",
    "        f\"Write a response that appropriately completes the request.\"\n",
    "        f\"\\n\\n### Instruction:\\n{entry['instruction']}\"\n",
    "    )\n",
    "\n",
    "    input_text = f\"\\n\\n### Input:\\n{entry['input']}\" if entry[\"input\"] else \"\"\n",
    "    return instruction_text + input_text\n",
    "\n",
    "\n",
    "model_input = format_input(data[50])\n",
    "desired_response = f\"\\n\\n### Response:\\n{data[50]['output']}\"\n",
    "\n",
    "print(model_input + desired_response)\n",
    "\n",
    "print()\n",
    "\n",
    "model_input = format_input(data[999])\n",
    "desired_response = f\"\\n\\n### Response:\\n{data[999]['output']}\"\n",
    "\n",
    "print(model_input + desired_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set length: 935\n",
      "Validation set length: 55\n",
      "Test set length: 110\n"
     ]
    }
   ],
   "source": [
    "# 分配训练集和测试集\n",
    "train_portion = int(len(data) * 0.85)  # 85% for training\n",
    "test_portion = int(len(data) * 0.1)    # 10% for testing\n",
    "val_portion = len(data) - train_portion - test_portion  # Remaining 5% for validation\n",
    "\n",
    "train_data = data[:train_portion]\n",
    "test_data = data[train_portion:train_portion + test_portion]\n",
    "val_data = data[train_portion + test_portion:]\n",
    "\n",
    "print(\"Training set length:\", len(train_data))\n",
    "print(\"Validation set length:\", len(val_data))\n",
    "print(\"Test set length:\", len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50256]\n",
      "tensor([[    0,     1,     2,     3,     4],\n",
      "        [    5,     6, 50256, 50256, 50256],\n",
      "        [    7,     8,     9, 50256, 50256]])\n"
     ]
    }
   ],
   "source": [
    "import torch,tiktoken\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class InstructionDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer):\n",
    "        self.data = data\n",
    "\n",
    "        # Pre-tokenize texts\n",
    "        self.encoded_texts = []\n",
    "        for entry in data:\n",
    "            instruction_plus_input = format_input(entry)\n",
    "            response_text = f\"\\n\\n### Response:\\n{entry['output']}\"\n",
    "            full_text = instruction_plus_input + response_text\n",
    "            self.encoded_texts.append(\n",
    "                tokenizer.encode(full_text)\n",
    "            )\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.encoded_texts[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "print(tokenizer.encode(\"<|endoftext|>\", allowed_special={\"<|endoftext|>\"}))\n",
    "\n",
    "\n",
    "def custom_collate_draft_1(batch, pad_token_ids = 50256,device = \"cpu\",):\n",
    "    batch_max_length = max(len(item) + 1 for item in batch)\n",
    "\n",
    "    inputs_lst = []\n",
    "\n",
    "    for item in batch:\n",
    "        new_item = item.copy()\n",
    "        new_item += [pad_token_ids]\n",
    "\n",
    "        padded = (new_item + [pad_token_ids] * (batch_max_length - len (new_item)))\n",
    "\n",
    "        inputs = torch.tensor(padded[:-1])\n",
    "        inputs_lst.append(inputs)\n",
    "\n",
    "    inputs_tensor = torch.stack(inputs_lst).to(device)\n",
    "\n",
    "    return inputs_tensor\n",
    "\n",
    "\n",
    "## 测试填充\n",
    "inputs_1 = [0, 1, 2, 3, 4]\n",
    "inputs_2 = [5, 6]\n",
    "inputs_3 = [7, 8, 9]\n",
    "\n",
    "batch = (\n",
    "    inputs_1,\n",
    "    inputs_2,\n",
    "    inputs_3\n",
    ")\n",
    "\n",
    "print(custom_collate_draft_1(batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    0,     1,     2,     3,     4],\n",
      "        [    5,     6, 50256, 50256, 50256],\n",
      "        [    7,     8,     9, 50256, 50256]])\n",
      "tensor([[    1,     2,     3,     4, 50256],\n",
      "        [    6, 50256, 50256, 50256, 50256],\n",
      "        [    8,     9, 50256, 50256, 50256]])\n"
     ]
    }
   ],
   "source": [
    "# 这里的训练和预训练有些类似，因此结构从1开始\n",
    "def custom_collate_draft_2(\n",
    "    batch,\n",
    "    pad_token_id=50256,\n",
    "    device=\"cpu\",\n",
    "):\n",
    "    batch_max_length = max(len(item) + 1 for item in batch)\n",
    "    inputs_lst, targets_lst = [], []\n",
    "    for item in batch:\n",
    "        new_item = item.copy()\n",
    "        # Add an <|endoftext|> token\n",
    "        new_item += [pad_token_id]\n",
    "        # Pad sequences to max_length\n",
    "        padded = new_item + [pad_token_id] * (batch_max_length - len(new_item))\n",
    "        inputs = torch.tensor(padded[:-1])  # Truncate the last token for inputs\n",
    "        targets = torch.tensor(padded[1:])  # Shift +1 to the right for targets\n",
    "        inputs_lst.append(inputs)\n",
    "        targets_lst.append(targets)\n",
    "\n",
    "    inputs_tensor = torch.stack(inputs_lst).to(device)\n",
    "    targets_tensor = torch.stack(targets_lst).to(device)\n",
    "    return inputs_tensor, targets_tensor\n",
    "\n",
    "\n",
    "inputs, targets = custom_collate_draft_2(batch)\n",
    "print(inputs)\n",
    "print(targets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    0,     1,     2,     3,     4],\n",
      "        [    5,     6, 50256, 50256, 50256],\n",
      "        [    7,     8,     9, 50256, 50256]])\n",
      "tensor([[    1,     2,     3,     4, 50256],\n",
      "        [    6, 50256,  -100,  -100,  -100],\n",
      "        [    8,     9, 50256,  -100,  -100]])\n"
     ]
    }
   ],
   "source": [
    "def custom_collate_fn(\n",
    "    batch,\n",
    "    pad_token_id=50256,\n",
    "    ignore_index=-100,\n",
    "    allowed_max_length=None,\n",
    "    device=\"cpu\"\n",
    "):\n",
    "    batch_max_length = max(len(item)+1 for item in batch)\n",
    "    inputs_lst, targets_lst = [], []\n",
    "    for item in batch:\n",
    "        new_item = item.copy()\n",
    "        # Add an <|endoftext|> token\n",
    "        new_item += [pad_token_id]\n",
    "        \n",
    "        # Pad sequences to max_length\n",
    "        padded = (\n",
    "            new_item + [pad_token_id] *\n",
    "            (batch_max_length - len(new_item))\n",
    "        )\n",
    "        inputs = torch.tensor(padded[:-1])  # Truncate the last token for inputs\n",
    "        targets = torch.tensor(padded[1:])  # Shift +1 to the right for targets\n",
    "\n",
    "        # New: Replace all but the first padding tokens in targets by ignore_index\n",
    "        mask = targets == pad_token_id\n",
    "        indices = torch.nonzero(mask).squeeze()\n",
    "        if indices.numel() > 1:\n",
    "            targets[indices[1:]] = ignore_index\n",
    "\n",
    "        # New: Optionally truncate to maximum sequence length\n",
    "        if allowed_max_length is not None:\n",
    "            inputs = inputs[:allowed_max_length]\n",
    "            targets = targets[:allowed_max_length]\n",
    "\n",
    "        inputs_lst.append(inputs)\n",
    "        targets_lst.append(targets)\n",
    "\n",
    "    # Convert list of inputs and targets to tensors and transfer to target device\n",
    "    inputs_tensor = torch.stack(inputs_lst).to(device)\n",
    "    targets_tensor = torch.stack(targets_lst).to(device)\n",
    "\n",
    "    return inputs_tensor, targets_tensor\n",
    "\n",
    "\n",
    "inputs, targets = custom_collate_fn(batch)\n",
    "print(inputs)\n",
    "print(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.1269)\n",
      "tensor(0.7936)\n",
      "tensor(1.1269)\n",
      "loss_1 == loss_3: tensor(True)\n"
     ]
    }
   ],
   "source": [
    "logits_1 = torch.tensor([[-1.0, 1.0], [-0.5, 1.5]])\n",
    "\n",
    "targets_1 = torch.tensor([0, 1])\n",
    "\n",
    "loss_1 = torch.nn.functional.cross_entropy(logits_1, targets_1)\n",
    "print(loss_1)\n",
    "\n",
    "logits_2 = torch.tensor(\n",
    "    [[-1.0, 1.0],\n",
    "     [-0.5, 1.5],\n",
    "     [-0.5, 1.5]]  # New 3rd training example\n",
    ")\n",
    "targets_2 = torch.tensor([0, 1, 1])\n",
    "\n",
    "loss_2 = torch.nn.functional.cross_entropy(logits_2, targets_2)\n",
    "print(loss_2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## 默认情况下，PyTorch具有cross_entropy(..., ignore_index=-100)设置，用于忽略与标签-100相对应的样本。\n",
    "targets_3 = torch.tensor([0, 1, -100])\n",
    "\n",
    "loss_3 = torch.nn.functional.cross_entropy(logits_2, targets_3)\n",
    "print(loss_3)\n",
    "print(\"loss_1 == loss_3:\", loss_1 == loss_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loader:\n",
      "torch.Size([8, 61]) torch.Size([8, 61])\n",
      "torch.Size([8, 76]) torch.Size([8, 76])\n",
      "torch.Size([8, 73]) torch.Size([8, 73])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 65]) torch.Size([8, 65])\n",
      "torch.Size([8, 72]) torch.Size([8, 72])\n",
      "torch.Size([8, 80]) torch.Size([8, 80])\n",
      "torch.Size([8, 67]) torch.Size([8, 67])\n",
      "torch.Size([8, 62]) torch.Size([8, 62])\n",
      "torch.Size([8, 75]) torch.Size([8, 75])\n",
      "torch.Size([8, 62]) torch.Size([8, 62])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 67]) torch.Size([8, 67])\n",
      "torch.Size([8, 77]) torch.Size([8, 77])\n",
      "torch.Size([8, 69]) torch.Size([8, 69])\n",
      "torch.Size([8, 79]) torch.Size([8, 79])\n",
      "torch.Size([8, 71]) torch.Size([8, 71])\n",
      "torch.Size([8, 66]) torch.Size([8, 66])\n",
      "torch.Size([8, 83]) torch.Size([8, 83])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 80]) torch.Size([8, 80])\n",
      "torch.Size([8, 71]) torch.Size([8, 71])\n",
      "torch.Size([8, 69]) torch.Size([8, 69])\n",
      "torch.Size([8, 65]) torch.Size([8, 65])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 60]) torch.Size([8, 60])\n",
      "torch.Size([8, 59]) torch.Size([8, 59])\n",
      "torch.Size([8, 69]) torch.Size([8, 69])\n",
      "torch.Size([8, 63]) torch.Size([8, 63])\n",
      "torch.Size([8, 65]) torch.Size([8, 65])\n",
      "torch.Size([8, 76]) torch.Size([8, 76])\n",
      "torch.Size([8, 66]) torch.Size([8, 66])\n",
      "torch.Size([8, 71]) torch.Size([8, 71])\n",
      "torch.Size([8, 91]) torch.Size([8, 91])\n",
      "torch.Size([8, 65]) torch.Size([8, 65])\n",
      "torch.Size([8, 64]) torch.Size([8, 64])\n",
      "torch.Size([8, 67]) torch.Size([8, 67])\n",
      "torch.Size([8, 66]) torch.Size([8, 66])\n",
      "torch.Size([8, 64]) torch.Size([8, 64])\n",
      "torch.Size([8, 65]) torch.Size([8, 65])\n",
      "torch.Size([8, 75]) torch.Size([8, 75])\n",
      "torch.Size([8, 89]) torch.Size([8, 89])\n",
      "torch.Size([8, 59]) torch.Size([8, 59])\n",
      "torch.Size([8, 88]) torch.Size([8, 88])\n",
      "torch.Size([8, 83]) torch.Size([8, 83])\n",
      "torch.Size([8, 83]) torch.Size([8, 83])\n",
      "torch.Size([8, 70]) torch.Size([8, 70])\n",
      "torch.Size([8, 65]) torch.Size([8, 65])\n",
      "torch.Size([8, 74]) torch.Size([8, 74])\n",
      "torch.Size([8, 76]) torch.Size([8, 76])\n",
      "torch.Size([8, 67]) torch.Size([8, 67])\n",
      "torch.Size([8, 75]) torch.Size([8, 75])\n",
      "torch.Size([8, 83]) torch.Size([8, 83])\n",
      "torch.Size([8, 69]) torch.Size([8, 69])\n",
      "torch.Size([8, 67]) torch.Size([8, 67])\n",
      "torch.Size([8, 60]) torch.Size([8, 60])\n",
      "torch.Size([8, 60]) torch.Size([8, 60])\n",
      "torch.Size([8, 66]) torch.Size([8, 66])\n",
      "torch.Size([8, 80]) torch.Size([8, 80])\n",
      "torch.Size([8, 71]) torch.Size([8, 71])\n",
      "torch.Size([8, 61]) torch.Size([8, 61])\n",
      "torch.Size([8, 58]) torch.Size([8, 58])\n",
      "torch.Size([8, 71]) torch.Size([8, 71])\n",
      "torch.Size([8, 67]) torch.Size([8, 67])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 63]) torch.Size([8, 63])\n",
      "torch.Size([8, 87]) torch.Size([8, 87])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 64]) torch.Size([8, 64])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 71]) torch.Size([8, 71])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 71]) torch.Size([8, 71])\n",
      "torch.Size([8, 61]) torch.Size([8, 61])\n",
      "torch.Size([8, 65]) torch.Size([8, 65])\n",
      "torch.Size([8, 67]) torch.Size([8, 67])\n",
      "torch.Size([8, 65]) torch.Size([8, 65])\n",
      "torch.Size([8, 64]) torch.Size([8, 64])\n",
      "torch.Size([8, 60]) torch.Size([8, 60])\n",
      "torch.Size([8, 72]) torch.Size([8, 72])\n",
      "torch.Size([8, 64]) torch.Size([8, 64])\n",
      "torch.Size([8, 70]) torch.Size([8, 70])\n",
      "torch.Size([8, 57]) torch.Size([8, 57])\n",
      "torch.Size([8, 72]) torch.Size([8, 72])\n",
      "torch.Size([8, 64]) torch.Size([8, 64])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 62]) torch.Size([8, 62])\n",
      "torch.Size([8, 74]) torch.Size([8, 74])\n",
      "torch.Size([8, 80]) torch.Size([8, 80])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 70]) torch.Size([8, 70])\n",
      "torch.Size([8, 91]) torch.Size([8, 91])\n",
      "torch.Size([8, 61]) torch.Size([8, 61])\n",
      "torch.Size([8, 66]) torch.Size([8, 66])\n",
      "torch.Size([8, 80]) torch.Size([8, 80])\n",
      "torch.Size([8, 81]) torch.Size([8, 81])\n",
      "torch.Size([8, 74]) torch.Size([8, 74])\n",
      "torch.Size([8, 82]) torch.Size([8, 82])\n",
      "torch.Size([8, 63]) torch.Size([8, 63])\n",
      "torch.Size([8, 83]) torch.Size([8, 83])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 67]) torch.Size([8, 67])\n",
      "torch.Size([8, 77]) torch.Size([8, 77])\n",
      "torch.Size([8, 91]) torch.Size([8, 91])\n",
      "torch.Size([8, 64]) torch.Size([8, 64])\n",
      "torch.Size([8, 61]) torch.Size([8, 61])\n",
      "torch.Size([8, 75]) torch.Size([8, 75])\n",
      "torch.Size([8, 64]) torch.Size([8, 64])\n",
      "torch.Size([8, 66]) torch.Size([8, 66])\n",
      "torch.Size([8, 78]) torch.Size([8, 78])\n",
      "torch.Size([8, 66]) torch.Size([8, 66])\n",
      "torch.Size([8, 64]) torch.Size([8, 64])\n",
      "torch.Size([8, 83]) torch.Size([8, 83])\n",
      "torch.Size([8, 66]) torch.Size([8, 66])\n",
      "torch.Size([8, 74]) torch.Size([8, 74])\n",
      "torch.Size([8, 69]) torch.Size([8, 69])\n",
      "tensor([  318,   281, 12064,   326,  8477,   257,  4876,    13, 19430,   257,\n",
      "         2882,   326, 20431, 32543,   262,  2581,    13,   198,   198, 21017,\n",
      "        46486,    25,   198, 30003,  6525,   262,  6827,  1262,   257,   985,\n",
      "          576,    13,   198,   198, 21017, 23412,    25,   198,   464,  5156,\n",
      "          318,   845, 13779,    13,   198,   198, 21017, 18261,    25,   198,\n",
      "          464,  5156,   318,   355, 13779,   355,   257,  4936,    13, 50256,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100])\n"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "\n",
    "customized_collate_fn = partial(\n",
    "    custom_collate_fn, device=\"cpu\", allowed_max_length=1024\n",
    ")\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "num_workers = 0\n",
    "batch_size = 8\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "train_dataset = InstructionDataset(train_data, tokenizer)\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=customized_collate_fn,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    num_workers=num_workers,\n",
    ")\n",
    "\n",
    "val_dataset = InstructionDataset(val_data, tokenizer)\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=customized_collate_fn,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    num_workers=num_workers,\n",
    ")\n",
    "\n",
    "test_dataset = InstructionDataset(test_data, tokenizer)\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=customized_collate_fn,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    num_workers=num_workers,\n",
    ")\n",
    "print(\"Train loader:\")\n",
    "for inputs, targets in train_loader:\n",
    "    print(inputs.shape, targets.shape)\n",
    "\n",
    "print(targets[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-10 00:26:56.562295: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-12-10 00:26:56.562744: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-12-10 00:26:56.565156: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-12-10 00:26:56.571469: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1733761616.581640 2822066 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1733761616.584601 2822066 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-10 00:26:56.595711: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "ename": "URLError",
     "evalue": "<urlopen error [Errno 104] Connection reset by peer>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mConnectionResetError\u001b[0m                      Traceback (most recent call last)",
      "File \u001b[0;32m~/J1ee/implement-llms-from-scratch/.conda/lib/python3.12/urllib/request.py:1344\u001b[0m, in \u001b[0;36mAbstractHTTPHandler.do_open\u001b[0;34m(self, http_class, req, **http_conn_args)\u001b[0m\n\u001b[1;32m   1343\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1344\u001b[0m     \u001b[43mh\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselector\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1345\u001b[0m \u001b[43m              \u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhas_header\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mTransfer-encoding\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1346\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err: \u001b[38;5;66;03m# timeout error\u001b[39;00m\n",
      "File \u001b[0;32m~/J1ee/implement-llms-from-scratch/.conda/lib/python3.12/http/client.py:1336\u001b[0m, in \u001b[0;36mHTTPConnection.request\u001b[0;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[1;32m   1335\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Send a complete request to the server.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1336\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/J1ee/implement-llms-from-scratch/.conda/lib/python3.12/http/client.py:1382\u001b[0m, in \u001b[0;36mHTTPConnection._send_request\u001b[0;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[1;32m   1381\u001b[0m     body \u001b[38;5;241m=\u001b[39m _encode(body, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbody\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m-> 1382\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mendheaders\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencode_chunked\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/J1ee/implement-llms-from-scratch/.conda/lib/python3.12/http/client.py:1331\u001b[0m, in \u001b[0;36mHTTPConnection.endheaders\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CannotSendHeader()\n\u001b[0;32m-> 1331\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_output\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencode_chunked\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/J1ee/implement-llms-from-scratch/.conda/lib/python3.12/http/client.py:1091\u001b[0m, in \u001b[0;36mHTTPConnection._send_output\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1090\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_buffer[:]\n\u001b[0;32m-> 1091\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1093\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m message_body \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1094\u001b[0m \n\u001b[1;32m   1095\u001b[0m     \u001b[38;5;66;03m# create a consistent interface to message_body\u001b[39;00m\n",
      "File \u001b[0;32m~/J1ee/implement-llms-from-scratch/.conda/lib/python3.12/http/client.py:1035\u001b[0m, in \u001b[0;36mHTTPConnection.send\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1034\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_open:\n\u001b[0;32m-> 1035\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1036\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/J1ee/implement-llms-from-scratch/.conda/lib/python3.12/http/client.py:1477\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1475\u001b[0m     server_hostname \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhost\n\u001b[0;32m-> 1477\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrap_socket\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1478\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver_hostname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/J1ee/implement-llms-from-scratch/.conda/lib/python3.12/ssl.py:455\u001b[0m, in \u001b[0;36mSSLContext.wrap_socket\u001b[0;34m(self, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, session)\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrap_socket\u001b[39m(\u001b[38;5;28mself\u001b[39m, sock, server_side\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    450\u001b[0m                 do_handshake_on_connect\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    451\u001b[0m                 suppress_ragged_eofs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    452\u001b[0m                 server_hostname\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, session\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    453\u001b[0m     \u001b[38;5;66;03m# SSLSocket class handles server_hostname encoding before it calls\u001b[39;00m\n\u001b[1;32m    454\u001b[0m     \u001b[38;5;66;03m# ctx._wrap_socket()\u001b[39;00m\n\u001b[0;32m--> 455\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msslsocket_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    456\u001b[0m \u001b[43m        \u001b[49m\u001b[43msock\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m        \u001b[49m\u001b[43mserver_side\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver_side\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    458\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdo_handshake_on_connect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdo_handshake_on_connect\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    459\u001b[0m \u001b[43m        \u001b[49m\u001b[43msuppress_ragged_eofs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msuppress_ragged_eofs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    460\u001b[0m \u001b[43m        \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver_hostname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    461\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    462\u001b[0m \u001b[43m        \u001b[49m\u001b[43msession\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msession\u001b[49m\n\u001b[1;32m    463\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/J1ee/implement-llms-from-scratch/.conda/lib/python3.12/ssl.py:1041\u001b[0m, in \u001b[0;36mSSLSocket._create\u001b[0;34m(cls, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, context, session)\u001b[0m\n\u001b[1;32m   1040\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdo_handshake_on_connect should not be specified for non-blocking sockets\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1041\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo_handshake\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1042\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n",
      "File \u001b[0;32m~/J1ee/implement-llms-from-scratch/.conda/lib/python3.12/ssl.py:1319\u001b[0m, in \u001b[0;36mSSLSocket.do_handshake\u001b[0;34m(self, block)\u001b[0m\n\u001b[1;32m   1318\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msettimeout(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m-> 1319\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo_handshake\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1320\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[0;31mConnectionResetError\u001b[0m: [Errno 104] Connection reset by peer",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mURLError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 27\u001b[0m\n\u001b[1;32m     24\u001b[0m BASE_CONFIG\u001b[38;5;241m.\u001b[39mupdate(model_configs[CHOOSE_MODEL])\n\u001b[1;32m     26\u001b[0m model_size \u001b[38;5;241m=\u001b[39m CHOOSE_MODEL\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mlstrip(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mrstrip(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 27\u001b[0m settings, params \u001b[38;5;241m=\u001b[39m \u001b[43mdownload_and_load_gpt2\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodels_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgpt2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m     30\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m model \u001b[38;5;241m=\u001b[39m GPTModel(BASE_CONFIG)\n\u001b[1;32m     33\u001b[0m load_weights_into_gpt(model, params)\n",
      "File \u001b[0;32m~/J1ee/implement-llms-from-scratch/ch7/gpt_download.py:37\u001b[0m, in \u001b[0;36mdownload_and_load_gpt2\u001b[0;34m(model_size, models_dir)\u001b[0m\n\u001b[1;32m     35\u001b[0m     file_url \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(base_url, model_size, filename)\n\u001b[1;32m     36\u001b[0m     file_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(model_dir, filename)\n\u001b[0;32m---> 37\u001b[0m     \u001b[43mdownload_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# Load settings and params\u001b[39;00m\n\u001b[1;32m     40\u001b[0m tf_ckpt_path \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mtrain\u001b[38;5;241m.\u001b[39mlatest_checkpoint(model_dir)\n",
      "File \u001b[0;32m~/J1ee/implement-llms-from-scratch/ch7/gpt_download.py:51\u001b[0m, in \u001b[0;36mdownload_file\u001b[0;34m(url, destination)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdownload_file\u001b[39m(url, destination):\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;66;03m# Send a GET request to download the file\u001b[39;00m\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43murllib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m response:\n\u001b[1;32m     52\u001b[0m             \u001b[38;5;66;03m# Get the total file size from headers, defaulting to 0 if not present\u001b[39;00m\n\u001b[1;32m     53\u001b[0m             file_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(response\u001b[38;5;241m.\u001b[39mheaders\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mContent-Length\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m0\u001b[39m))\n\u001b[1;32m     55\u001b[0m             \u001b[38;5;66;03m# Check if file exists and has the same size\u001b[39;00m\n",
      "File \u001b[0;32m~/J1ee/implement-llms-from-scratch/.conda/lib/python3.12/urllib/request.py:215\u001b[0m, in \u001b[0;36murlopen\u001b[0;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    214\u001b[0m     opener \u001b[38;5;241m=\u001b[39m _opener\n\u001b[0;32m--> 215\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mopener\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/J1ee/implement-llms-from-scratch/.conda/lib/python3.12/urllib/request.py:515\u001b[0m, in \u001b[0;36mOpenerDirector.open\u001b[0;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[1;32m    512\u001b[0m     req \u001b[38;5;241m=\u001b[39m meth(req)\n\u001b[1;32m    514\u001b[0m sys\u001b[38;5;241m.\u001b[39maudit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124murllib.Request\u001b[39m\u001b[38;5;124m'\u001b[39m, req\u001b[38;5;241m.\u001b[39mfull_url, req\u001b[38;5;241m.\u001b[39mdata, req\u001b[38;5;241m.\u001b[39mheaders, req\u001b[38;5;241m.\u001b[39mget_method())\n\u001b[0;32m--> 515\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    517\u001b[0m \u001b[38;5;66;03m# post-process response\u001b[39;00m\n\u001b[1;32m    518\u001b[0m meth_name \u001b[38;5;241m=\u001b[39m protocol\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_response\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/J1ee/implement-llms-from-scratch/.conda/lib/python3.12/urllib/request.py:532\u001b[0m, in \u001b[0;36mOpenerDirector._open\u001b[0;34m(self, req, data)\u001b[0m\n\u001b[1;32m    529\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[1;32m    531\u001b[0m protocol \u001b[38;5;241m=\u001b[39m req\u001b[38;5;241m.\u001b[39mtype\n\u001b[0;32m--> 532\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_chain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_open\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprotocol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprotocol\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\n\u001b[1;32m    533\u001b[0m \u001b[43m                          \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m_open\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    534\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result:\n\u001b[1;32m    535\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/J1ee/implement-llms-from-scratch/.conda/lib/python3.12/urllib/request.py:492\u001b[0m, in \u001b[0;36mOpenerDirector._call_chain\u001b[0;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[1;32m    490\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m handler \u001b[38;5;129;01min\u001b[39;00m handlers:\n\u001b[1;32m    491\u001b[0m     func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(handler, meth_name)\n\u001b[0;32m--> 492\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    493\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    494\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/J1ee/implement-llms-from-scratch/.conda/lib/python3.12/urllib/request.py:1392\u001b[0m, in \u001b[0;36mHTTPSHandler.https_open\u001b[0;34m(self, req)\u001b[0m\n\u001b[1;32m   1391\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhttps_open\u001b[39m(\u001b[38;5;28mself\u001b[39m, req):\n\u001b[0;32m-> 1392\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhttp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mHTTPSConnection\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1393\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_context\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/J1ee/implement-llms-from-scratch/.conda/lib/python3.12/urllib/request.py:1347\u001b[0m, in \u001b[0;36mAbstractHTTPHandler.do_open\u001b[0;34m(self, http_class, req, **http_conn_args)\u001b[0m\n\u001b[1;32m   1344\u001b[0m         h\u001b[38;5;241m.\u001b[39mrequest(req\u001b[38;5;241m.\u001b[39mget_method(), req\u001b[38;5;241m.\u001b[39mselector, req\u001b[38;5;241m.\u001b[39mdata, headers,\n\u001b[1;32m   1345\u001b[0m                   encode_chunked\u001b[38;5;241m=\u001b[39mreq\u001b[38;5;241m.\u001b[39mhas_header(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTransfer-encoding\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m   1346\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err: \u001b[38;5;66;03m# timeout error\u001b[39;00m\n\u001b[0;32m-> 1347\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m URLError(err)\n\u001b[1;32m   1348\u001b[0m     r \u001b[38;5;241m=\u001b[39m h\u001b[38;5;241m.\u001b[39mgetresponse()\n\u001b[1;32m   1349\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n",
      "\u001b[0;31mURLError\u001b[0m: <urlopen error [Errno 104] Connection reset by peer>"
     ]
    }
   ],
   "source": [
    "# 加载模型\n",
    "\n",
    "\n",
    "from gpt_download import download_and_load_gpt2\n",
    "from previous_chapters import GPTModel, load_weights_into_gpt\n",
    "\n",
    "\n",
    "BASE_CONFIG = {\n",
    "    \"vocab_size\": 50257,     # Vocabulary size\n",
    "    \"context_length\": 1024,  # Context length\n",
    "    \"drop_rate\": 0.0,        # Dropout rate\n",
    "    \"qkv_bias\": True         # Query-key-value bias\n",
    "}\n",
    "\n",
    "model_configs = {\n",
    "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
    "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
    "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
    "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
    "}\n",
    "\n",
    "CHOOSE_MODEL = \"gpt2-medium (355M)\"\n",
    "\n",
    "BASE_CONFIG.update(model_configs[CHOOSE_MODEL])\n",
    "\n",
    "model_size = CHOOSE_MODEL.split(\" \")[-1].lstrip(\"(\").rstrip(\")\")\n",
    "settings, params = download_and_load_gpt2(\n",
    "    model_size=model_size,\n",
    "    models_dir=\"gpt2\"\n",
    ")\n",
    "\n",
    "model = GPTModel(BASE_CONFIG)\n",
    "load_weights_into_gpt(model, params)\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Convert the active sentence to passive: 'The chef cooks the meal every day.'\n",
      "The chef cooks the meal every day.\n",
      "\n",
      "### Instruction:\n",
      "\n",
      "Convert the active sentence to passive: 'The chef cooks the\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "input_text = format_input(val_data[0])\n",
    "print(input_text)\n",
    "\n",
    "from previous_chapters import (\n",
    "    generate,\n",
    "    text_to_token_ids,\n",
    "    token_ids_to_text\n",
    ")\n",
    "token_ids = generate(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(input_text, tokenizer),\n",
    "    max_new_tokens=35,\n",
    "    context_size=BASE_CONFIG[\"context_length\"],\n",
    "    eos_id=50256,\n",
    ")\n",
    "\n",
    "generated_text = token_ids_to_text(token_ids, tokenizer)\n",
    "\n",
    "response_text = (\n",
    "    generated_text[len(input_text):]\n",
    "    .replace(\"### Response:\", \"\")\n",
    "    .strip()\n",
    ")\n",
    "\n",
    "print(response_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 3.8258957386016847\n",
      "Validation loss: 3.7619208812713625\n"
     ]
    }
   ],
   "source": [
    "## 下面开始正式微调了,先计算损失\n",
    "from previous_chapters import (\n",
    "    calc_loss_loader,\n",
    "    train_model_simple\n",
    ")\n",
    "device = \"cpu\"\n",
    "model.to(device)\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "with torch.no_grad():\n",
    "    train_loss = calc_loss_loader(train_loader, model, device, num_batches=5)\n",
    "    val_loss = calc_loss_loader(val_loader, model, device, num_batches=5)\n",
    "\n",
    "print(\"Training loss:\", train_loss)\n",
    "print(\"Validation loss:\", val_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1 (Step 000000): Train loss 2.637, Val loss 2.626\n",
      "Ep 1 (Step 000005): Train loss 1.174, Val loss 1.102\n",
      "Ep 1 (Step 000010): Train loss 0.872, Val loss 0.945\n",
      "Ep 1 (Step 000015): Train loss 0.856, Val loss 0.906\n",
      "Ep 1 (Step 000020): Train loss 0.776, Val loss 0.881\n",
      "Ep 1 (Step 000025): Train loss 0.753, Val loss 0.859\n",
      "Ep 1 (Step 000030): Train loss 0.798, Val loss 0.836\n",
      "Ep 1 (Step 000035): Train loss 0.714, Val loss 0.808\n",
      "Ep 1 (Step 000040): Train loss 0.672, Val loss 0.806\n",
      "Ep 1 (Step 000045): Train loss 0.633, Val loss 0.790\n",
      "Ep 1 (Step 000050): Train loss 0.662, Val loss 0.783\n",
      "Ep 1 (Step 000055): Train loss 0.760, Val loss 0.764\n",
      "Ep 1 (Step 000060): Train loss 0.719, Val loss 0.743\n",
      "Ep 1 (Step 000065): Train loss 0.652, Val loss 0.735\n",
      "Ep 1 (Step 000070): Train loss 0.532, Val loss 0.729\n",
      "Ep 1 (Step 000075): Train loss 0.569, Val loss 0.729\n",
      "Ep 1 (Step 000080): Train loss 0.605, Val loss 0.725\n",
      "Ep 1 (Step 000085): Train loss 0.509, Val loss 0.709\n",
      "Ep 1 (Step 000090): Train loss 0.562, Val loss 0.691\n",
      "Ep 1 (Step 000095): Train loss 0.500, Val loss 0.681\n",
      "Ep 1 (Step 000100): Train loss 0.502, Val loss 0.677\n",
      "Ep 1 (Step 000105): Train loss 0.564, Val loss 0.670\n",
      "Ep 1 (Step 000110): Train loss 0.555, Val loss 0.667\n",
      "Ep 1 (Step 000115): Train loss 0.508, Val loss 0.664\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Convert the active sentence to passive: 'The chef cooks the meal every day.'  ### Response: The meal is prepared every day by the chef.<|endoftext|>The following is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Convert the active sentence to passive:\n",
      "Ep 2 (Step 000120): Train loss 0.435, Val loss 0.672\n",
      "Ep 2 (Step 000125): Train loss 0.450, Val loss 0.687\n",
      "Ep 2 (Step 000130): Train loss 0.447, Val loss 0.682\n",
      "Ep 2 (Step 000135): Train loss 0.405, Val loss 0.681\n",
      "Ep 2 (Step 000140): Train loss 0.409, Val loss 0.681\n",
      "Ep 2 (Step 000145): Train loss 0.368, Val loss 0.681\n",
      "Ep 2 (Step 000150): Train loss 0.381, Val loss 0.676\n",
      "Ep 2 (Step 000155): Train loss 0.412, Val loss 0.676\n",
      "Ep 2 (Step 000160): Train loss 0.415, Val loss 0.684\n",
      "Ep 2 (Step 000165): Train loss 0.379, Val loss 0.686\n",
      "Ep 2 (Step 000170): Train loss 0.323, Val loss 0.683\n",
      "Ep 2 (Step 000175): Train loss 0.337, Val loss 0.671\n",
      "Ep 2 (Step 000180): Train loss 0.392, Val loss 0.658\n",
      "Ep 2 (Step 000185): Train loss 0.415, Val loss 0.660\n",
      "Ep 2 (Step 000190): Train loss 0.340, Val loss 0.651\n",
      "Ep 2 (Step 000195): Train loss 0.330, Val loss 0.638\n",
      "Ep 2 (Step 000200): Train loss 0.310, Val loss 0.638\n",
      "Ep 2 (Step 000205): Train loss 0.351, Val loss 0.634\n",
      "Ep 2 (Step 000210): Train loss 0.366, Val loss 0.632\n",
      "Ep 2 (Step 000215): Train loss 0.396, Val loss 0.636\n",
      "Ep 2 (Step 000220): Train loss 0.300, Val loss 0.648\n",
      "Ep 2 (Step 000225): Train loss 0.348, Val loss 0.660\n",
      "Ep 2 (Step 000230): Train loss 0.294, Val loss 0.657\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Convert the active sentence to passive: 'The chef cooks the meal every day.'  ### Response: The meal is cooked every day by the chef.<|endoftext|>The following is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: What is the capital of the United Kingdom\n",
      "Training completed in 31.00 minutes.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "torch.manual_seed(123)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.00005, weight_decay=0.1)\n",
    "num_epochs = 2 ## 迭代两次\n",
    "\n",
    "train_losses, val_losses, tokens_seen = train_model_simple(\n",
    "    model, train_loader, val_loader, optimizer, device,\n",
    "    num_epochs=num_epochs, eval_freq=5, eval_iter=5,\n",
    "    start_context=format_input(val_data[0]), tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time_minutes = (end_time - start_time) / 60\n",
    "print(f\"Training completed in {execution_time_minutes:.2f} minutes.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from previous_chapters import plot_losses\n",
    "\n",
    "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
    "plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Rewrite the sentence using a simile.\n",
      "\n",
      "### Input:\n",
      "The car is very fast.\n",
      "\n",
      "Correct response:\n",
      ">> The car is as fast as lightning.\n",
      "\n",
      "Model response:\n",
      ">> The car is as fast as a bullet.\n",
      "-------------------------------------\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "What type of cloud is typically associated with thunderstorms?\n",
      "\n",
      "Correct response:\n",
      ">> The type of cloud typically associated with thunderstorms is cumulonimbus.\n",
      "\n",
      "Model response:\n",
      ">> The type of cloud associated with thunderstorms is a cumulus cloud.\n",
      "-------------------------------------\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Name the author of 'Pride and Prejudice'.\n",
      "\n",
      "Correct response:\n",
      ">> Jane Austen.\n",
      "\n",
      "Model response:\n",
      ">> The author of 'Pride and Prejudice' is Jane Austen.\n",
      "-------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# 最后测试一下\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "\n",
    "for entry in test_data[:3]:\n",
    "\n",
    "    input_text = format_input(entry)\n",
    "\n",
    "    token_ids = generate(\n",
    "        model=model,\n",
    "        idx=text_to_token_ids(input_text, tokenizer).to(device),\n",
    "        max_new_tokens=256,\n",
    "        context_size=BASE_CONFIG[\"context_length\"],\n",
    "        eos_id=50256\n",
    "    )\n",
    "    generated_text = token_ids_to_text(token_ids, tokenizer)\n",
    "    response_text = (\n",
    "        generated_text[len(input_text):]\n",
    "        .replace(\"### Response:\", \"\")\n",
    "        .strip()\n",
    ")\n",
    "\n",
    "    print(input_text)\n",
    "    print(f\"\\nCorrect response:\\n>> {entry['output']}\")\n",
    "    print(f\"\\nModel response:\\n>> {response_text.strip()}\")\n",
    "    print(\"-------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 110/110 [08:40<00:00,  4.73s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "for i, entry in tqdm(enumerate(test_data), total=len(test_data)):\n",
    "\n",
    "    input_text = format_input(entry)\n",
    "\n",
    "    token_ids = generate(\n",
    "        model=model,\n",
    "        idx=text_to_token_ids(input_text, tokenizer).to(device),\n",
    "        max_new_tokens=256,\n",
    "        context_size=BASE_CONFIG[\"context_length\"],\n",
    "        eos_id=50256\n",
    "    )\n",
    "    generated_text = token_ids_to_text(token_ids, tokenizer)\n",
    "    response_text = generated_text[len(input_text):].replace(\"### Response:\", \"\").strip()\n",
    "\n",
    "    test_data[i][\"model_response\"] = response_text\n",
    "\n",
    "\n",
    "with open(\"instruction-data-with-response.json\", \"w\") as file:\n",
    "    json.dump(test_data, file, indent=4)  # \"indent\" for pretty-printing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved as gpt2-medium355M-sft.pth\n"
     ]
    }
   ],
   "source": [
    "# 保存模型\n",
    "import re\n",
    "\n",
    "\n",
    "file_name = f\"{re.sub(r'[ ()]', '', CHOOSE_MODEL) }-sft.pth\"\n",
    "torch.save(model.state_dict(), file_name)\n",
    "print(f\"Model saved as {file_name}\")\n",
    "\n",
    "# Load model via\n",
    "# model.load_state_dict(torch.load(\"gpt2-medium355M-sft.pth\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 评估微调后的模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Llamas are herbivores, which means they primarily eat plants and plant-based foods. Their diet typically consists of:\n",
      "\n",
      "1. Grasses: Llamas love to graze on various types of grasses, including tall grasses, short grasses, and even weeds.\n",
      "2. Hay: High-quality hay, such as alfalfa or timothy hay, is a staple in many llama diets.\n",
      "3. Grains: Oats, barley, and corn are common grains that llamas enjoy.\n",
      "4. Fruits and vegetables: Llamas might eat fruits like apples, carrots, and sweet potatoes, as well as leafy greens like kale and spinach.\n",
      "5. Minerals: Llamas need access to minerals like calcium, phosphorus, and salt to stay healthy.\n",
      "\n",
      "In the wild, llamas would typically roam freely, eating whatever plants are available in their natural habitat. In captivity, llama owners often provide a balanced diet that includes a mix of these foods, along with fresh water, to keep their animals happy and healthy.\n",
      "\n",
      "It's worth noting that llamas have a unique digestive system that allows them to digest plant material more efficiently than many other animals. They can even eat plants that are toxic to other animals! However, it's still important for llama owners to provide a well-balanced diet and ensure their animals have access to clean water at all times.\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "import json\n",
    "def query_model(\n",
    "    prompt,\n",
    "    model=\"llama3\",\n",
    "    url=\"http://localhost:11434/api/chat\"\n",
    "):\n",
    "    # Create the data payload as a dictionary\n",
    "    data = {\n",
    "        \"model\": model,\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        \"options\": {     # Settings below are required for deterministic responses\n",
    "            \"seed\": 123,\n",
    "            \"temperature\": 0,\n",
    "            \"num_ctx\": 2048\n",
    "        }\n",
    "    }\n",
    "\n",
    "\n",
    "    # Convert the dictionary to a JSON formatted string and encode it to bytes\n",
    "    payload = json.dumps(data).encode(\"utf-8\")\n",
    "\n",
    "    # Create a request object, setting the method to POST and adding necessary headers\n",
    "    request = urllib.request.Request(\n",
    "        url,\n",
    "        data=payload,\n",
    "        method=\"POST\"\n",
    "    )\n",
    "    request.add_header(\"Content-Type\", \"application/json\")\n",
    "\n",
    "    # Send the request and capture the response\n",
    "    response_data = \"\"\n",
    "    with urllib.request.urlopen(request) as response:\n",
    "        # Read and decode the response\n",
    "        while True:\n",
    "            line = response.readline().decode(\"utf-8\")\n",
    "            if not line:\n",
    "                break\n",
    "            response_json = json.loads(line)\n",
    "            response_data += response_json[\"message\"][\"content\"]\n",
    "\n",
    "    return response_data\n",
    "\n",
    "\n",
    "model = \"llama3\"\n",
    "result = query_model(\"What do Llamas eat?\", model)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'model_response'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m entry \u001b[38;5;129;01min\u001b[39;00m test_data[:\u001b[38;5;241m3\u001b[39m]:\n\u001b[1;32m      2\u001b[0m     prompt \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m      3\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGiven the input `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mformat_input(entry)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      4\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mand correct output `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mentry[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 5\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscore the model response `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mentry\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodel_response\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      6\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m on a scale from 0 to 100, where 100 is the best score. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      7\u001b[0m     )\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mDataset response:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m>>\u001b[39m\u001b[38;5;124m\"\u001b[39m, entry[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[0;31mKeyError\u001b[0m: 'model_response'"
     ]
    }
   ],
   "source": [
    "for entry in test_data[:3]:\n",
    "    prompt = (\n",
    "        f\"Given the input `{format_input(entry)}` \"\n",
    "        f\"and correct output `{entry['output']}`, \"\n",
    "        f\"score the model response `{entry['model_response']}`\"\n",
    "        f\" on a scale from 0 to 100, where 100 is the best score. \"\n",
    "    )\n",
    "    print(\"\\nDataset response:\")\n",
    "    print(\">>\", entry['output'])\n",
    "    print(\"\\nModel response:\")\n",
    "    print(\">>\", entry[\"model_response\"])\n",
    "    print(\"\\nScore:\")\n",
    "    print(\">>\", query_model(prompt))\n",
    "    print(\"\\n-------------------------\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
